---
title: "Machine Learning Assignment"
author: "Abu Nayeem"
date: "September 19, 2014"
output: html_document
---

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

#####Executive Summary

Technology has focused on developing health tools and gadgets to record how much training a person has done in a specific period of time. However, almost no research has been done in developing tools or models to give the trainer feedback on how well he has been performing exercises. This project is oriented in calculating a machine learning algorithm to determine whether a weight lifting trainer performed the exercise well or made an erro in the execution. A Random Forest algorithm is applied and cross validated to avoid over fitting. It is conclusive that the model performs with 99% accuracy and may be utilized to give feedback to weight lifting trainers.  

Spceial thanks and data came from the paper below:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Weight Lifting Exercises Dataset:
The dataset measures the performance of six individuals performing various exercises [A-E], where A is the correct way of doing the exercise, while B to E are common mistakes in doing an exercise. The four main censors include sensors in the belt, forarm, arm (biceps), and the dumbell itself.
To learn more about the collection process: read here

For the assignment we were given a large training set [19622,160] and a small testing set to test the prediction model made from the training set

### Preparation

Loading libraries:
```{r, message=FALSE}
if (!file.exists("./out-of-box-samples")) {
     dir.create("./out-of-box-samples")
}
rm(list = ls(all = TRUE))
library(caret) # machine learning package
library(plyr) # data table operations
library(dplyr) # data operations plus
library(gbm) # general boosting method
library(sampling) # for sampling putposes
date() #set date
```

Extract Respective Data: [Note: I changed NA and #Div/0 into NA strings to make it easier to data clean later]
```{r, results='hide'}
trainingfile <- 'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
training <- read.csv(trainingfile, na.strings=c("NA", "#DIV/0!"))
training <- tbl_df(training) # this data table is smoother
testingfile <- 'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
testing <- read.csv(testingfile, na.strings=c("NA", "#DIV/0!"))
testing <- tbl_df(testing)
dim(training)
str(training)
```

### Data Cleaning: 

The goal is to make the data simple/small as possible without losing predictability. The primary strategy in datamining is that I cannot change the training set fundamentally because the testing set will not have those changes because it will be raw. So the goal of the datamining process if finding which columns are primary canndidates to implement the analysis. In addition, this prevents me from changing classes of variables, because the predictor alogithm may not work for the raw dataset.

1) I notice alot of variables that are missing values, I handle missing values the following way 
```{r, results='hide'}
colSums(is.na(training)) # now we see the number of missing values in columns and see if they are significant for removal
NonNAIndex <- which(colSums(is.na(training)) > 0) # this extracts the index of missing variable
RemoveNA <- training[ ,-NonNAIndex] # Create new data frame that remmove columns that had missing values
compacttraining <- select(RemoveNA, 2:5, 8:60) # choose the columns that may be useful for analysis
```

2) Test if there are zero covariates:
```{r}
Nsv <- nearZeroVar(compacttraining,saveMetrics=TRUE) # this checks if all columns have close to zero variance 
# the saveMetric provide heuristic information of each column which is REALLY useful
Nsv # all false, so no columns will be removed
```

3) Check for correlated pairs
```{r, results='hide'}
M <- abs(cor(compacttraining[ ,c(2, 3, 5:56)]))
diag(M) <- 0
which(M > 0.8, arr.ind=T)
```

Matching Pairs: below
(30)magnet_arm_z & magnet_arm_y(29) 
(38)accel_dumbbell_x & pitch_dumbbell (31)
(40)accel_dumbbell_z & yaw_dumbbell (33) 
(28)magnet_arm_x & accel_arm_x(25) 
(22)gyros_arm_x & gyros_arm_y(23)
(15)magnet_belt_x => pitch_belt(6) & accel_belt_x [circle](12)
(5)roll_belt => & yaw_belt(7) & total_accel_belt(8) & accel_belt_y(13) & accel_belt_z(14) 
(50)gyros_forearm_z => gyros_forearm_y (49) & gyros_dumbbell_z(37) & gyros_dumbbell_x(35)

I do not use PCA to combine the variables because the analysis would need the testing set as well and the rows may not comply. So I simply remove columns that are in the right side of the pairing. The remainder variable would not be strongly correlated

Remove the following colums and save the column index: We can index it to the raw dataset
```{r}
descriptivetraining <- select(compacttraining, 
                       -c(magnet_arm_y , pitch_dumbbell, yaw_dumbbell , accel_arm_x, gyros_arm_y, 
                       pitch_belt, accel_belt_x, yaw_belt , total_accel_belt , accel_belt_y , 
                       accel_belt_z, gyros_forearm_y, gyros_dumbbell_z, gyros_dumbbell_x))
descriptivetraining <- select(compacttraining, -c(30,31,34,26,23,8,12,7,6,13,14,49,37,35)) # the same as above                     
columncompactIndex <- colnames(descriptivetraining)  # the important index!
```

###Data Preparation

I will be taking a random sample of 3000 observations. From this sample, I split it into a smaller training set and a cross validation set, which acts more or less like a testing set. Given the datacleaning process, the out of sample error should be small.
```{r}

sampletrain <- training[sample(nrow(training), 3000), ]
inTrain <- createDataPartition(y=sampletrain$classe, p=0.7, list=FALSE)
smalltraining <- sampletrain[inTrain, ]
crossvalidation <- sampletrain[-inTrain, ]
```

### Machine Learning Implementation

I used general boosting model with trees because the predictive value is factor not an integer. In addition, the boosting model works well because the number of features is plenty and there is obvious variables on interest and features in this model have weak relationship of preditor alone. Finally, the gbm is computationally efficient for machine.
```{r, results='hide'}
gbmGrid <-  expand.grid(interaction.depth = 5, # the number of interactions between features
                        n.trees = 150, # the total number of trees or iterations to be implemented
                        shrinkage = 0.1) # the learning rate of step-size function
modelFit <- train(classe ~ ., method="gbm", data=smalltraining[ ,columncompactIndex], tuneGrid = gbmGrid)
# fit the model
```

### The Results

The modelfit is then tested on the cross validation set 
```{r}
results<-predict(modelFit,crossvalidation) 
confusionMatrix(results,crossvalidation$classe)
```
The prediction is around 99% accuracy.

The following below show prediction of model in comparison to test set.
```{r, results='hide'}
Predict <- predict(modelFit,testing[,-160]) 
Predict
```


An indepth analysis of results and alernative will be used under the second update.
